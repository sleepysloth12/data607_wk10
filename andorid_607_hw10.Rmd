---
title: "Data 607 Week 10 Assignment"
author: "Jean Jimenez"
date: "2023-10-30"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


API Request and Obtaining Data

```{r}
#loading libraries
library(httr)
library(jsonlite)
library(tidyverse)

#api key
api_key = "g1JljwbIVFhBovObSkLwkJHfNfRDoPLo"


end_date = format(Sys.Date(), "%Y%m%d")
start_date = format(Sys.Date() - 90, "%Y%m%d")

#api request
#returns top articles in past 3 months
api_url = "https://api.nytimes.com/svc/search/v2/articlesearch.json"
params = list(
  "q" = "Benjamin Netanyahu",
  "begin_date" = start_date,
  "end_date" = end_date,
  "api-key" = api_key
)

#JSON response
response = GET(api_url, query = params)

#cleaning JSON Data
parsed_response = content(response, as="text")
json_data = fromJSON(parsed_response)

jr=json_data$response

jrd=jr$docs

names(json_data)
names(jr)
names(jr$docs)


ids=jrd$`_id`
urls=jrd$web_url
headlines=jrd$headline
snippets= jrd$snippet
abstracts=jrd$abstract
lead_paragraphs=jrd$lead_paragraph
word_counts=jrd$word_count

netan_dat=data.frame(
  url=urls,
  headline=headlines,
  snippet=snippets,
  abstract=abstracts,
  lead_paragraph=lead_paragraphs,
  word_count=word_counts)

netan_dat= netan_dat %>%
  select(url, headline.main, snippet, abstract, lead_paragraph, word_count)

netan_dat$lead_paragraph

colnames(netan_dat)=c("url","headline","snippet","abstract","lead_paragraph", "word_count")

#data frame to use
#exporting so we can all work on the same data.
write.csv(netan_dat, "netan_dat.csv")

```


Might not be enough Text in snippet or lead paragraph to do sentiment analysis.

I have NYT subscription.

Making a new column on the csv and and adding file prefix.

Uploading Full text to Github.

```{r}
#CSV from above
csv_link="https://raw.githubusercontent.com/sleepysloth12/data607_wk10/main/netan_dat.csv"

netan_dat=read.csv(url(csv_link))

#prefix for github full txt folder
full_txt_raw_url_prefix="https://raw.githubusercontent.com/sleepysloth12/data607_wk10/main/fullText/"

#full txt url column with link to full txt.
netan_dat= netan_dat %>%
  filter(article_text_file != "") %>%
  mutate(full_txt_url= paste0(full_txt_raw_url_prefix,article_text_file,".txt") )

#All full texts are in this data frame in the full_text column
netan_dat_txt = netan_dat %>% 
  rowwise() %>% 
  mutate(full_txt = list(readLines(full_txt_url))) %>%
  ungroup()



```


Tokenization

(I will do the first link and tokenize it by paragraph)
will work on it now.

```{r}
#First Article
library(tidytext)
library(textdata)
library(cleanNLP)


first_txt= netan_dat_txt$full_txt[1]
class(first_txt)

first_txt=first_txt[[1]]
tot_first_len=length(first_txt)

#getting each paragraph
first_article_paragraphs=c()


for (i in 1:tot_first_len){
  if(first_txt[i] != ""){
    first_article_paragraphs=c(first_article_paragraphs,first_txt[i])}
}

#all Paragraphs of the first article
first_article_paragraphs


first_df = tibble(paragraph = 1:length(first_article_paragraphs), text = first_article_paragraphs)

nrc_sentiments=get_sentiments("nrc")

cnlp_init_udpipe()
obj_1 = cnlp_annotate(first_df$text)

#NRC Results first article
sar_1_nrc = first_df %>%
  unnest_tokens(word, text) %>%
  inner_join(nrc_sentiments, by = "word") %>%
  group_by(paragraph, sentiment) %>%
  summarise(sentiment_count = n()) %>%
  spread(key = sentiment, value = sentiment_count, fill = 0) %>%
  mutate(net_sentiment = positive - negative)



```


Second Article.

Separate by sentence

```{r}
second_txt= netan_dat_txt$full_txt[2]
class(second_txt)

second_txt=second_txt[[1]]
tot_second_len=length(second_txt)

#getting each sentence
second_article_sentences=c()


for (i in 1:tot_second_len){
  if(second_txt[i] != ""){
    second_article_sentences=c(second_article_sentences,second_txt[i])}
}

#all Paragraphs of the 2nd article
second_article_sentences

second_article_sentences = lapply(second_article_sentences, function(x) unlist(str_split(x, "\\. ")))
second_article_sentences = unlist(second_article_sentences)


second_df = tibble(sentence = 1:length(second_article_sentences), text = second_article_sentences)


cnlp_init_udpipe()
obj_2 = cnlp_annotate(second_df$text)

#NRC Results second article
sar_2_nrc = second_df %>%
  unnest_tokens(word, text) %>%
  inner_join(nrc_sentiments, by = "word") %>%
  group_by(sentence, sentiment) %>%
  summarise(sentiment_count = n()) %>%
  spread(key = sentiment, value = sentiment_count, fill = 0) %>%
  mutate(net_sentiment = positive - negative)

```


Third Article

seperated by sentence 

```{r}
third_txt= netan_dat_txt$full_txt[3]
class(third_txt)

third_txt=third_txt[[1]]
tot_third_len=length(third_txt)

#getting each sentence
third_article_sentences=c()


for (i in 1:tot_third_len){
  if(third_txt[i] != ""){
    third_article_sentences=c(third_article_sentences,third_txt[i])}
}

#all Paragraphs of the 3rd article
third_article_sentences

third_article_sentences = lapply(third_article_sentences, function(x) unlist(str_split(x, "\\. ")))
third_article_sentences = unlist(third_article_sentences)


third_df = tibble(sentence = 1:length(third_article_sentences), text = third_article_sentences)

obj_3 = cnlp_annotate(third_df$text)

#NRC Results 3rd article
sar_3_nrc = third_df %>%
  unnest_tokens(word, text) %>%
  inner_join(nrc_sentiments, by = "word") %>%
  group_by(sentence, sentiment) %>%
  summarise(sentiment_count = n()) %>%
  spread(key = sentiment, value = sentiment_count, fill = 0) %>%
  mutate(net_sentiment = positive - negative)
```


Fourth Article

Separated by sentence

```{r}
fourth_txt= netan_dat_txt$full_txt[4]
class(fourth_txt)

fourth_txt=fourth_txt[[1]]
tot_fourth_len=length(fourth_txt)

#getting each sentence
fourth_article_sentences=c()


for (i in 1:tot_fourth_len){
  if(fourth_txt[i] != ""){
    fourth_article_sentences=c(fourth_article_sentences,fourth_txt[i])}
}

#all Paragraphs of the 4th article
fourth_article_sentences

fourth_article_sentences = lapply(fourth_article_sentences, function(x) unlist(str_split(x, "\\. ")))
fourth_article_sentences = unlist(fourth_article_sentences)


fourth_df = tibble(sentence = 1:length(fourth_article_sentences), text = fourth_article_sentences)

obj_4 = cnlp_annotate(fourth_df$text)

#NRC Results fourth article
sar_4_nrc = fourth_df %>%
  unnest_tokens(word, text) %>%
  inner_join(nrc_sentiments, by = "word") %>%
  group_by(sentence, sentiment) %>%
  summarise(sentiment_count = n()) %>%
  spread(key = sentiment, value = sentiment_count, fill = 0) %>%
  mutate(net_sentiment = positive - negative)
```



Fifth Article

Separated by Sentence

```{r}
fifth_txt= netan_dat_txt$full_txt[5]
class(fifth_txt)

fifth_txt=fifth_txt[[1]]
tot_fifth_len=length(fifth_txt)

#getting each sentence
fifth_article_sentences=c()


for (i in 1:tot_fifth_len){
  if(fifth_txt[i] != ""){
    fifth_article_sentences=c(fifth_article_sentences,fifth_txt[i])}
}

#all Paragraphs of the 5th article
fifth_article_sentences

fifth_article_sentences = lapply(fifth_article_sentences, function(x) unlist(str_split(x, "\\. ")))
fifth_article_sentences = unlist(fifth_article_sentences)


fifth_df = tibble(sentence = 1:length(fifth_article_sentences), text = fifth_article_sentences)

obj_5 = cnlp_annotate(fifth_df$text)

#NRC Results fifth article
sar_5_nrc = fifth_df %>%
  unnest_tokens(word, text) %>%
  inner_join(nrc_sentiments, by = "word") %>%
  group_by(sentence, sentiment) %>%
  summarise(sentiment_count = n()) %>%
  spread(key = sentiment, value = sentiment_count, fill = 0) %>%
  mutate(net_sentiment = positive - negative)
```

